{
    "docs": [
        {
            "location": "/",
            "text": "Historical\n\n\n\n\n\nThis project is in very active development and is not yet ready for production use!\n\n\n\nHistorical is a serverless application that reacts to and tracks changes made to AWS assets anywhere in\nyour environment. Historical achieves this by describing AWS assets when they are changed, and keeping the history of those changes along with the the CloudTrail context of those changes.\n\n\nHistorical persists data in two places:\n\n\n\n\nA \"Current\" DynamoDB table, which is a cache of the current state of AWS assets\n\n\nA \"Durable\" DynamoDB table, which stores the change history of AWS assets\n\n\n\n\nHistorical enables downstream consumers to react to changes in the AWS environment\nwithout the need to directly describe the asset. This greatly increases speed of reaction, reduces IAM permission\ncomplexity, and also avoids rate limiting.\n\n\nHow it works\n\u00b6\n\n\nHistorical leverages AWS CloudWatch Events. Events trigger a \"Collector\" Lambda function\ndescribes the AWS asset that changed and saves the configuration of the asset into a DynamoDB table. From this, a \"Differ\" Lambda function checks if the asset has effectively change from what was previously known about that asset. If the item has changed, a new change record is logged, which then enables downstream\nconsumers the ability to react to changes in the environment as the environment effectively changes.\n\n\nThe CloudTrail context on the change is preserved in all stages.\n\n\nCurrent Technologies Implemented\n\u00b6\n\n\n\n\n\n\nS3\n\u00b6\n\n\n\n\n\n\nSecurity Groups\n\u00b6\n\n\n\n\n\n\nArchitecture\n\u00b6\n\n\nPlease review the \nArchitecture\n documentation for an in-depth description of the components involved.\n\n\nInstallation & Configuration\n\u00b6\n\n\nPlease review the \nInstallation and Configuration\n documentation for additional details.",
            "title": "Welcome"
        },
        {
            "location": "/#how-it-works",
            "text": "Historical leverages AWS CloudWatch Events. Events trigger a \"Collector\" Lambda function\ndescribes the AWS asset that changed and saves the configuration of the asset into a DynamoDB table. From this, a \"Differ\" Lambda function checks if the asset has effectively change from what was previously known about that asset. If the item has changed, a new change record is logged, which then enables downstream\nconsumers the ability to react to changes in the environment as the environment effectively changes.  The CloudTrail context on the change is preserved in all stages.",
            "title": "How it works"
        },
        {
            "location": "/#current-technologies-implemented",
            "text": "",
            "title": "Current Technologies Implemented"
        },
        {
            "location": "/#s3",
            "text": "",
            "title": "S3"
        },
        {
            "location": "/#security-groups",
            "text": "",
            "title": "Security Groups"
        },
        {
            "location": "/#architecture",
            "text": "Please review the  Architecture  documentation for an in-depth description of the components involved.",
            "title": "Architecture"
        },
        {
            "location": "/#installation-configuration",
            "text": "Please review the  Installation and Configuration  documentation for additional details.",
            "title": "Installation &amp; Configuration"
        },
        {
            "location": "/architecture/",
            "text": "Historical Architecture\n\u00b6\n\n\nHistorical is a serverless application in AWS. It consists of many components and are described in this document.\n\n\nHistorical is written in Python 3 and heavily leverages AWS technologies such as Lambda, SNS, SQS, DynamoDB, CloudTrail, and CloudWatch.\n\n\nGeneral Architectural Overview\n\u00b6\n\n\nHere is a diagram of the Historical Architecture:\n\n\n\nPlease Note:\n This stack is deployed \nfor every technology monitored\n! There are many, many Historical stacks that will be deployed!\n\n\nPolling vs. Events\n\u00b6\n\n\nHistorical is \nboth\n a polling and event driven system. It will periodically poll AWS accounts for changes. Because Historical responds to events in the environment, polling doesn't need to be very aggressive.\n\n\nUnfortunately, events are not 100% reliable, and as such, polling is required to describe items that may have had lost events.\n\n\nHistorical is \neventually consistent\n, and makes a \nbest effort\n to maintain a current and up-to-date inventory of assets.\n\n\nPrerequisites\n\u00b6\n\n\nFor all of this to work the following prerequisites that must be satisfied:\n\n\n\n\nALL AWS\n accounts must be configured to send CloudWatch events over a CloudWatch Event Bus to the Historical AWS account.\n\n\nALL AWS accounts\n and \nALL regions\n in those accounts need to have a CloudWatch Event rule that captures ALL events and sends them over the Event Bus.\n\n\nALL AWS\n accounts must have CloudTrail enabled.\n\n\nA \nHistorical\n IAM role must exist in \nALL AWS\n accounts with permissions that are defined \nhere\n*(TODO ADD THESE!). This role must have an \nAssumeRolePolicyDocument\n to permit access from the \nHistoricalLambdaProfile\n IAM role in the Historical Account.\n\n\nHistorical makes use of \nSWAG\n to define which AWS accounts Historical is enabled for. SWAG must be properly configured for Historical to operate.\n\n\n\n\nThe CloudWatch configuration is outlined here:\n\n\n\nThe IAM configuration is outlined here:\nTODO ADD ME\n\n\nRegions\n\u00b6\n\n\nHistorical has the concept of regions that fit 3 categories:\n\n\n\n\nPrimary region\n\n\nSecondary region(s)\n\n\nOff region(s)\n\n\n\n\nThe \nPrimary Region\n is considered the \"Base\" of Historical. This region has all the major components that make up Historical. This region is responsible for getting events from ALL the off-regions -- which are regions that don't require a full Historical stack, but would still like to receive events from.\n\n\nThe \nSecondary Region(s)\n are regions that are important to you. Secondary regions look like the primary region, and process events locally. If you have a lot of infrastructure in a region, you should place a Historical stack there. This will allow you to quickly receive and process events, and also gives your applications a regionally-local means of accessing Historical data.\n\n\nThe \nOff Region(s)\n are regions you don't have a lot of infrastructure deployed in. However, you still want visibility in these regions should events happen there. These regions have very minimal amount of infrastructure deployed. These regions will forward ALL events to the Primary Region for processing.\n\n\nNote:\n It is highly recommended to have a Historical off-region stack in any region that is not Primary or Secondary. This will ensure full visibility in your environment.\n\n\nComponent Overview\n\u00b6\n\n\nThis section describes some of the high-level architectural components.\n\n\nPrimary Components\n\u00b6\n\n\nBelow are the primary components of the Historical architecture:\n\n\n\n\nCloudWatch Event Rules\n\n\nCloudWatch Change Events\n\n\nPoller\n\n\nCollector\n\n\nCurrent Table\n\n\nDynamoDB Stream Proxy\n\n\nDiffer\n\n\nDurable Table\n\n\nOff-region SNS forwarders\n\n\n\n\nAs general overview, the infrastructure follows a pipeline from start to finish. An event will arrive, will get enriched with additional information, and will provide notifications to downstream infrastructure on the given changes.\n\n\nSQS queues are used in as many places as possible to trigger Lambda functions. SQS makes it easy to provide Lambda execution concurrency, with retry on failure as well as dead-letter queuing capabilities.\n\n\nSNS topics are used to make it easy for \nN\n number of interested parties to subscribe to the Historical DynamoDB tables as they become updated. Presently, this is only attached to the Durable table. More details on this below.\n\n\nCloudWatch Event Rules\n\u00b6\n\n\nThere are two different CloudWatch Event Rules:\n\n\n\n\nTimed Events\n\n\nChange Events\n\n\n\n\nTimed events are used to kick off the Poller. See the section on the poller below for additional details. Change events are events that arrive from an asset in the AWS environment undergoing a change.\n\n\nPoller\n\u00b6\n\n\nThe Poller's primary function is to obtain a full inventory of AWS assets.\n\n\nThe Poller is split into two parts:\n\n\n\n\nPoller Tasker\n\n\nPoller\n\n\n\n\nThe \"Poller Tasker\" is a Lambda function that iterates over all AWS accounts Historical is configured for, and will task the Poller to \nlist\n all assets in the given environment.\n\n\nThe Poller Tasker in the \nPRIMARY REGION\n will task the Poller to list assets that reside in the primary region and all off-regions. A Poller Tasker in a \nSECONDARY REGION\n will only task a poller to describe assets that reside in the same region.\n\n\nThe Poller \nlists\n all assets in a given account/region, and will task a \"Poller Collector\" to fetch details about the asset in question.\n\n\nCollector\n\u00b6\n\n\nThe Collector's primary function is to describe a given AWS asset and store its configuration to the \"Current\" DynamoDB table.\n\n\nThe Collector is split into two parts (same code, different execution triggers):\n\n\n\n\nPoller Collector\n\n\nEvent Collector\n\n\n\n\nThe Poller Collector is a collector that will only respond to polling events. The Event Collector will only respond to CloudWatch change events.\n\n\nThe Collector is split into two parts in order to prevent change events from being sandwiched in between polling events. Historical will always try to give priority to change events over polling events to ensure timeliness of asset configuration changes.\n\n\nIn both cases, the Collector will go to the AWS account and region that the item resides in, and use \nboto3\n to describe the configuration of the asset.\n\n\nCurrent Table\n\u00b6\n\n\nThe \"Current\" table is a global DynamoDB table that stores the current configuration of a given asset in\nAWS.\n\n\nThis acts as a cache for current the state of the environment.\n\n\nThe Current table has as DynamoDB stream that will kick off the DynamoDB Stream Proxy, which will then trigger the Differ.\n\n\nSpecial Note:\n\u00b6\n\n\nThe Current table has a TTL set on all items. This TTL is updated any time a change event arrives, or when the poller runs. The TTL is set to clean-up orphaned items, which can happen if a deletion event is lost. Deleted items will not be picked up by the Poller (only lists items that exist in the account) and thus, will be removed from the Current table on TTL expiration.\n\n\nDynamoDB Stream Proxy\n\u00b6\n\n\nThe DynamoDB Stream Proxy is a Lambda function that proxies DynamoDB stream events to SNS or SQS. The purpose is to task subsequent Lambda functions on the specific changes that happen to the DynamoDB table.\n\n\nThe Historical infrastructure has two configurations for the DynamoDB forwarder:\n\n\n\n\nCurrent Table Forwarder (DynamoDB Stream Proxy to Differ SQS)\n\n\nDurable Table Forwarder (DynamoDB Stream Proxy to Change Notification SNS)\n\n\n\n\nThe Current Table Forwarder proxies events to the SQS queue that triggers the Differ Lambda function. This is sent to SQS to speed up the time to trigger the Differ. SNS lacks a batch message sending API, and thus sending to SNS is slower as a result.\n\n\nThe Durable Table Forwarder proxies events to an SNS topic that any downstream subscriber for effective infrastructure changes can react to. SNS enables \nN\n subscribers to events.\n\n\nSpecial Note:\n\u00b6\n\n\nDynamoDB Streams in Global DynamoDB tables triggers this Lambda whenever a DynamoDB update occurs in ANY of the regions the table is configured to sync with. Thus, to avoid Historical Lambda functions from \"stepping on each other's toes\", the DynamoDB Stream Proxy has a \nPROXY_REGIONS\n environment variable. This variable (a comma-separated list of AWS regions) is configured to only proxy DynamoDB stream updates that occur to assets that are configured. The \nPRIMARY REGION\n will be configured to proxy events that occur in the primary region, and all off-regions. The \nSECONDARY REGION(S)\n will be configured to proxy events that occur in the same region.\n\n\nAnother Special Note:\n\u00b6\n\n\nDynamoDB items are capped to 400KB. SNS and SQS have maximum message sizes of 256KB. Logic exists to handle cases where DynamoDB items are too big to send over to SNS/SQS. Follow-up Lambdas and subscribers will need to make use of the Historical API to fetch the full configuration of the item either out of the Current or Durable tables (depending on the use case).\n\n\nDiffer\n\u00b6\n\n\nThe Differ is a Lambda function that gets triggered upon changes to the Current table. The Differ will check if the asset in question has had an effective change. If so, the Differ will save a new change record in the Durable table to maintain history of the asset as it changes over time, and will also save the CloudTrail context, including the time at which the event occurred.\n\n\nDurable Table\n\u00b6\n\n\nThe \"Durable\" table is a global DynamoDB table that stores an asset configuration with change history.\n\n\nThe Durable table has as DynamoDB stream that will kick off another DynamoDB Stream Proxy, which will be used for interested parties to react to effective changes that occur in the environment.\n\n\nOff-Region SNS Forwarders\n\u00b6\n\n\nIn off-regions, very bare infrastructure is intentionally deployed. This helps to reduce costs and complexity of the Historical infrastructure.\n\n\nThis is a SNS Topic that receives CloudWatch events for asset changes that occur in region. This topic  forwards events to the Event Collector SQS queue in the primary region.\n\n\nSpecial Stacks\n\u00b6\n\n\nSome asset types have different stack configurations due to nuances of the technology.\n\n\nThe following technologies have different stack types:\n\n\n\n\nS3\n\n\n\n\nS3\n\u00b6\n\n\nThe AWS S3 stack is almost identical to the standard stack. The difference is due to AWS S3 buckets having a globally unique namespace.\n\n\nFor S3, because it is not presently possible to only poll for in-region S3 buckets, the poller lives in the primary region only. The poller in the primary region polls for all S3 buckets in all regions.\n\n\nThe secondary regions will still respond to in-region events, but lack all polling components.\n\n\nThis diagram showcases the S3 stack.\n\n\nInstallation & Configuration\n\u00b6\n\n\nPlease refer to the installation docs for additional details.",
            "title": "Architecture"
        },
        {
            "location": "/architecture/#historical-architecture",
            "text": "Historical is a serverless application in AWS. It consists of many components and are described in this document.  Historical is written in Python 3 and heavily leverages AWS technologies such as Lambda, SNS, SQS, DynamoDB, CloudTrail, and CloudWatch.",
            "title": "Historical Architecture"
        },
        {
            "location": "/architecture/#general-architectural-overview",
            "text": "Here is a diagram of the Historical Architecture:  Please Note:  This stack is deployed  for every technology monitored ! There are many, many Historical stacks that will be deployed!",
            "title": "General Architectural Overview"
        },
        {
            "location": "/architecture/#polling-vs-events",
            "text": "Historical is  both  a polling and event driven system. It will periodically poll AWS accounts for changes. Because Historical responds to events in the environment, polling doesn't need to be very aggressive.  Unfortunately, events are not 100% reliable, and as such, polling is required to describe items that may have had lost events.  Historical is  eventually consistent , and makes a  best effort  to maintain a current and up-to-date inventory of assets.",
            "title": "Polling vs. Events"
        },
        {
            "location": "/architecture/#prerequisites",
            "text": "For all of this to work the following prerequisites that must be satisfied:   ALL AWS  accounts must be configured to send CloudWatch events over a CloudWatch Event Bus to the Historical AWS account.  ALL AWS accounts  and  ALL regions  in those accounts need to have a CloudWatch Event rule that captures ALL events and sends them over the Event Bus.  ALL AWS  accounts must have CloudTrail enabled.  A  Historical  IAM role must exist in  ALL AWS  accounts with permissions that are defined  here *(TODO ADD THESE!). This role must have an  AssumeRolePolicyDocument  to permit access from the  HistoricalLambdaProfile  IAM role in the Historical Account.  Historical makes use of  SWAG  to define which AWS accounts Historical is enabled for. SWAG must be properly configured for Historical to operate.   The CloudWatch configuration is outlined here:  The IAM configuration is outlined here:\nTODO ADD ME",
            "title": "Prerequisites"
        },
        {
            "location": "/architecture/#regions",
            "text": "Historical has the concept of regions that fit 3 categories:   Primary region  Secondary region(s)  Off region(s)   The  Primary Region  is considered the \"Base\" of Historical. This region has all the major components that make up Historical. This region is responsible for getting events from ALL the off-regions -- which are regions that don't require a full Historical stack, but would still like to receive events from.  The  Secondary Region(s)  are regions that are important to you. Secondary regions look like the primary region, and process events locally. If you have a lot of infrastructure in a region, you should place a Historical stack there. This will allow you to quickly receive and process events, and also gives your applications a regionally-local means of accessing Historical data.  The  Off Region(s)  are regions you don't have a lot of infrastructure deployed in. However, you still want visibility in these regions should events happen there. These regions have very minimal amount of infrastructure deployed. These regions will forward ALL events to the Primary Region for processing.  Note:  It is highly recommended to have a Historical off-region stack in any region that is not Primary or Secondary. This will ensure full visibility in your environment.",
            "title": "Regions"
        },
        {
            "location": "/architecture/#component-overview",
            "text": "This section describes some of the high-level architectural components.",
            "title": "Component Overview"
        },
        {
            "location": "/architecture/#primary-components",
            "text": "Below are the primary components of the Historical architecture:   CloudWatch Event Rules  CloudWatch Change Events  Poller  Collector  Current Table  DynamoDB Stream Proxy  Differ  Durable Table  Off-region SNS forwarders   As general overview, the infrastructure follows a pipeline from start to finish. An event will arrive, will get enriched with additional information, and will provide notifications to downstream infrastructure on the given changes.  SQS queues are used in as many places as possible to trigger Lambda functions. SQS makes it easy to provide Lambda execution concurrency, with retry on failure as well as dead-letter queuing capabilities.  SNS topics are used to make it easy for  N  number of interested parties to subscribe to the Historical DynamoDB tables as they become updated. Presently, this is only attached to the Durable table. More details on this below.",
            "title": "Primary Components"
        },
        {
            "location": "/architecture/#cloudwatch-event-rules",
            "text": "There are two different CloudWatch Event Rules:   Timed Events  Change Events   Timed events are used to kick off the Poller. See the section on the poller below for additional details. Change events are events that arrive from an asset in the AWS environment undergoing a change.",
            "title": "CloudWatch Event Rules"
        },
        {
            "location": "/architecture/#poller",
            "text": "The Poller's primary function is to obtain a full inventory of AWS assets.  The Poller is split into two parts:   Poller Tasker  Poller   The \"Poller Tasker\" is a Lambda function that iterates over all AWS accounts Historical is configured for, and will task the Poller to  list  all assets in the given environment.  The Poller Tasker in the  PRIMARY REGION  will task the Poller to list assets that reside in the primary region and all off-regions. A Poller Tasker in a  SECONDARY REGION  will only task a poller to describe assets that reside in the same region.  The Poller  lists  all assets in a given account/region, and will task a \"Poller Collector\" to fetch details about the asset in question.",
            "title": "Poller"
        },
        {
            "location": "/architecture/#collector",
            "text": "The Collector's primary function is to describe a given AWS asset and store its configuration to the \"Current\" DynamoDB table.  The Collector is split into two parts (same code, different execution triggers):   Poller Collector  Event Collector   The Poller Collector is a collector that will only respond to polling events. The Event Collector will only respond to CloudWatch change events.  The Collector is split into two parts in order to prevent change events from being sandwiched in between polling events. Historical will always try to give priority to change events over polling events to ensure timeliness of asset configuration changes.  In both cases, the Collector will go to the AWS account and region that the item resides in, and use  boto3  to describe the configuration of the asset.",
            "title": "Collector"
        },
        {
            "location": "/architecture/#current-table",
            "text": "The \"Current\" table is a global DynamoDB table that stores the current configuration of a given asset in\nAWS.  This acts as a cache for current the state of the environment.  The Current table has as DynamoDB stream that will kick off the DynamoDB Stream Proxy, which will then trigger the Differ.",
            "title": "Current Table"
        },
        {
            "location": "/architecture/#special-note",
            "text": "The Current table has a TTL set on all items. This TTL is updated any time a change event arrives, or when the poller runs. The TTL is set to clean-up orphaned items, which can happen if a deletion event is lost. Deleted items will not be picked up by the Poller (only lists items that exist in the account) and thus, will be removed from the Current table on TTL expiration.",
            "title": "Special Note:"
        },
        {
            "location": "/architecture/#dynamodb-stream-proxy",
            "text": "The DynamoDB Stream Proxy is a Lambda function that proxies DynamoDB stream events to SNS or SQS. The purpose is to task subsequent Lambda functions on the specific changes that happen to the DynamoDB table.  The Historical infrastructure has two configurations for the DynamoDB forwarder:   Current Table Forwarder (DynamoDB Stream Proxy to Differ SQS)  Durable Table Forwarder (DynamoDB Stream Proxy to Change Notification SNS)   The Current Table Forwarder proxies events to the SQS queue that triggers the Differ Lambda function. This is sent to SQS to speed up the time to trigger the Differ. SNS lacks a batch message sending API, and thus sending to SNS is slower as a result.  The Durable Table Forwarder proxies events to an SNS topic that any downstream subscriber for effective infrastructure changes can react to. SNS enables  N  subscribers to events.",
            "title": "DynamoDB Stream Proxy"
        },
        {
            "location": "/architecture/#special-note_1",
            "text": "DynamoDB Streams in Global DynamoDB tables triggers this Lambda whenever a DynamoDB update occurs in ANY of the regions the table is configured to sync with. Thus, to avoid Historical Lambda functions from \"stepping on each other's toes\", the DynamoDB Stream Proxy has a  PROXY_REGIONS  environment variable. This variable (a comma-separated list of AWS regions) is configured to only proxy DynamoDB stream updates that occur to assets that are configured. The  PRIMARY REGION  will be configured to proxy events that occur in the primary region, and all off-regions. The  SECONDARY REGION(S)  will be configured to proxy events that occur in the same region.",
            "title": "Special Note:"
        },
        {
            "location": "/architecture/#another-special-note",
            "text": "DynamoDB items are capped to 400KB. SNS and SQS have maximum message sizes of 256KB. Logic exists to handle cases where DynamoDB items are too big to send over to SNS/SQS. Follow-up Lambdas and subscribers will need to make use of the Historical API to fetch the full configuration of the item either out of the Current or Durable tables (depending on the use case).",
            "title": "Another Special Note:"
        },
        {
            "location": "/architecture/#differ",
            "text": "The Differ is a Lambda function that gets triggered upon changes to the Current table. The Differ will check if the asset in question has had an effective change. If so, the Differ will save a new change record in the Durable table to maintain history of the asset as it changes over time, and will also save the CloudTrail context, including the time at which the event occurred.",
            "title": "Differ"
        },
        {
            "location": "/architecture/#durable-table",
            "text": "The \"Durable\" table is a global DynamoDB table that stores an asset configuration with change history.  The Durable table has as DynamoDB stream that will kick off another DynamoDB Stream Proxy, which will be used for interested parties to react to effective changes that occur in the environment.",
            "title": "Durable Table"
        },
        {
            "location": "/architecture/#off-region-sns-forwarders",
            "text": "In off-regions, very bare infrastructure is intentionally deployed. This helps to reduce costs and complexity of the Historical infrastructure.  This is a SNS Topic that receives CloudWatch events for asset changes that occur in region. This topic  forwards events to the Event Collector SQS queue in the primary region.",
            "title": "Off-Region SNS Forwarders"
        },
        {
            "location": "/architecture/#special-stacks",
            "text": "Some asset types have different stack configurations due to nuances of the technology.  The following technologies have different stack types:   S3",
            "title": "Special Stacks"
        },
        {
            "location": "/architecture/#s3",
            "text": "The AWS S3 stack is almost identical to the standard stack. The difference is due to AWS S3 buckets having a globally unique namespace.  For S3, because it is not presently possible to only poll for in-region S3 buckets, the poller lives in the primary region only. The poller in the primary region polls for all S3 buckets in all regions.  The secondary regions will still respond to in-region events, but lack all polling components.  This diagram showcases the S3 stack.",
            "title": "S3"
        },
        {
            "location": "/architecture/#installation-configuration",
            "text": "Please refer to the installation docs for additional details.",
            "title": "Installation &amp; Configuration"
        },
        {
            "location": "/installation/",
            "text": "Installation & Configuration\n\u00b6\n\n\nThere is a lot to this... so please stay tuned!",
            "title": "Installation and Configuration"
        },
        {
            "location": "/installation/#installation-configuration",
            "text": "There is a lot to this... so please stay tuned!",
            "title": "Installation &amp; Configuration"
        }
    ]
}