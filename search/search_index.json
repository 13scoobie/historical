{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Historical This project is in very active development and is not yet ready for production use! Historical is a serverless application that tracks and reacts to AWS resource modifications anywhere in your environment. Historical achieves this by describing AWS resources when they are changed, and keeping the history of those changes along with the the CloudTrail context of those changes. Historical persists data in two places: A \"Current\" DynamoDB table, which is a cache of the current state of AWS resources A \"Durable\" DynamoDB table, which stores the change history of AWS resources Historical enables downstream consumers to react to changes in the AWS environment without the need to directly describe the resource. This greatly increases speed of reaction, reduces IAM permission complexity, and also avoids rate limiting. How it works \u00b6 Historical leverages AWS CloudWatch Events. Events trigger a \"Collector\" Lambda function to describe the AWS resource that changed, and saves the configuration into a DynamoDB table. From this, a \"Differ\" Lambda function checks if the resource has changed from what was previously known about that resource. If the item has changed, a new change record is saved, which then enables downstream consumers the ability to react to changes in the environment as the environment effectively changes over time. The CloudTrail context on the change is preserved in the change history. Current Technologies Implemented \u00b6 S3 \u00b6 Security Groups \u00b6 IAM (In active development -- Coming Soon!) \u00b6 Architecture \u00b6 Please review the Architecture documentation for an in-depth description of the components involved. Installation & Configuration \u00b6 Please review the Installation & Configuration documentation for details. Troubleshooting \u00b6 Please review the Troubleshooting doc if you are experiencing issues.","title":"Welcome"},{"location":"#how-it-works","text":"Historical leverages AWS CloudWatch Events. Events trigger a \"Collector\" Lambda function to describe the AWS resource that changed, and saves the configuration into a DynamoDB table. From this, a \"Differ\" Lambda function checks if the resource has changed from what was previously known about that resource. If the item has changed, a new change record is saved, which then enables downstream consumers the ability to react to changes in the environment as the environment effectively changes over time. The CloudTrail context on the change is preserved in the change history.","title":"How it works"},{"location":"#current-technologies-implemented","text":"","title":"Current Technologies Implemented"},{"location":"#s3","text":"","title":"S3"},{"location":"#security-groups","text":"","title":"Security Groups"},{"location":"#iam-in-active-development-coming-soon","text":"","title":"IAM (In active development -- Coming Soon!)"},{"location":"#architecture","text":"Please review the Architecture documentation for an in-depth description of the components involved.","title":"Architecture"},{"location":"#installation-configuration","text":"Please review the Installation & Configuration documentation for details.","title":"Installation &amp; Configuration"},{"location":"#troubleshooting","text":"Please review the Troubleshooting doc if you are experiencing issues.","title":"Troubleshooting"},{"location":"architecture/","text":"Historical Architecture \u00b6 Historical is a serverless AWS application that consists of many components. Historical is written in Python 3 and heavily leverages AWS technologies such as Lambda, SNS, SQS, DynamoDB, CloudTrail, and CloudWatch. General Architectural Overview \u00b6 Here is a diagram of the Historical Architecture: Please Note: This stack is deployed for every technology monitored ! There are many, many Historical stacks that will be deployed. Polling vs. Events \u00b6 Historical is both a polling and event driven system. It will periodically poll AWS accounts for changes. However, because Historical responds to events in the environment, polling doesn't need to be very aggressive and only happens once every few hours. Polling is necessary because events are not 100% reliable. This ensures that data is current just in case an event is dropped. Historical is eventually consistent , and makes a best effort to maintain a current and up-to-date inventory of AWS resources. Prerequisite Overview \u00b6 This is a high-level overview of the prerequisites that are required to make Historical operate. For more details on setting up the required prerequisites, please review the installation documentation . ALL AWS accounts accounts have CloudTrail enabled. ALL AWS accounts and ALL regions in those accounts have a CloudWatch Event rule that captures ALL events and sends them over the CloudWatch Event Bus to the Historical account for processing. IAM roles exist in ALL accounts and are assumable by the Historical Lambda functions. Historical makes use of SWAG to define which AWS accounts Historical is enabled for. While not a hard requirement, use of SWAG is highly recommended . Regions \u00b6 Historical has the concept of regions that fit 3 categories: Primary region Secondary region(s) Off region(s) The Primary Region is considered the \"Base\" of Historical. This region has all of the major components that make up Historical. This region processes all in-region AND off-region originating events. The Off Region(s) are regions you don't have a lot of infrastructure deployed in. However, you still want visibility in these regions should events happen there. These regions have very minimal amount of Historical-related infrastructure deployed. These regions will forward ALL events to the Primary Region for processing. The Secondary Region(s) are regions that are important to you. Secondary regions look like the primary region and process in-region events. If you have a lot of infrastructure within a region, you should place a Historical stack there. This will allow you to quickly receive and process events, and also gives your applications a regionally-local means of accessing Historical data. Note: Place a Historical off-region stack in any region that is not Primary or Secondary. This will ensure full visibility in your environment. Component Overview \u00b6 This section describes some of the high-level architectural components. Primary Components \u00b6 Below are the primary components of the Historical architecture: CloudWatch Event Rules CloudWatch Change Events Poller Collector Current Table DynamoDB Stream Proxy Differ Durable Table Off-region SNS forwarders As general overview, the infrastructure is an event processing and enriching pipeline. An event will arrive, will get enriched with additional information, and will provide notifications to downstream subscribers on the given changes. SQS queues are used in as many places as much as possible to invoke Lambda functions. SQS makes it easy to provide Lambda execution concurrency, auto-scaling, retry of failures without blocking, and dead-letter queuing capabilities. SNS topics are used to make it easy for N number of interested parties to subscribe to the Historical DynamoDB table changes. Presently, this is only attached to the Durable table. More details on this below. CloudWatch Event Rules \u00b6 There are two different CloudWatch Event Rules: Timed Events Change Events Timed events are used to kick off the Poller. See the section on the poller below for additional details. Change events are events that arrive from CloudWatch Events when an AWS resource's configuration changes. Poller \u00b6 The Poller's primary function is to obtain a full inventory of AWS resources. The Poller is split into two parts: Poller Tasker Poller The \"Poller Tasker\" is a Lambda function that iterates over all AWS accounts Historical is configured for, and tasks the Poller to list all resources in the given environment. The Poller Tasker in the PRIMARY REGION tasks the Poller to list resources that reside in the primary region and all off-regions. A Poller Tasker in a SECONDARY REGION will only task a poller to describe resources that reside in the same region. The Poller lists all resources in a given account/region, and tasks a \"Poller Collector\" to fetch details about the resource in question. Collector \u00b6 The Collector describes a given AWS resource and stores its configuration to the \"Current\" DynamoDB table. The Collector is split into two parts (same code, different invocation mechanisms): Poller Collector Event Collector The Poller Collector is a collector that will only respond to polling events. The Event Collector will only respond to CloudWatch change events. The Collector is split into two parts to prevent change events from being sandwiched in between polling events. Historical gives priority to change events over polling events to ensure timeliness of resource configuration changes. In both cases, the Collector will go to the AWS account and region that the item resides in, and use boto3 to describe the configuration of the resource. Current Table \u00b6 The \"Current\" table is a global DynamoDB table that stores the current configuration of a given resource in AWS. This acts as a cache for current the state of the environment. The Current table has as DynamoDB Stream that will kick off a DynamoDB Stream Proxy that then invokes the Differ. Special Note: \u00b6 The Current table has a TTL set on all items. This TTL is updated any time a change event arrives, or when the Poller runs. The TTL is set to clean-up orphaned items, which can happen if a deletion event is lost. Deleted items will not be picked up by the Poller (only lists items that exist in the account) and thus, will be removed from the Current table on TTL expiration. As a result, the Poller must \"see\" a resource at least once every few hours before it is deemed deleted from the environment. DynamoDB Stream Proxy \u00b6 The DynamoDB Stream Proxy is a Lambda function that proxies DynamoDB Stream events to SNS or SQS. The purpose is to task subsequent Lambda functions on the specific changes that happen to the DynamoDB table. The Historical infrastructure has two configurations for the DynamoDB Proxy: Current Table Forwarder (DynamoDB Stream Proxy to Differ SQS) Durable Table Forwarder (DynamoDB Stream Proxy to Change Notification SNS) The Current Table Forwarder proxies events to the SQS queue that invokes the Differ Lambda function. The Durable Table Forwarder proxies events to an SNS topic that can be subscribed. SNS enables N subscribers to Historical events. The Durable table proxy serializes the DynamoDB Stream events into an easily consumable JSON that contains the full and complete configuration of the resource in question, along with the the CloudTrail context. This enables downstream applications to make intelligent decisions about the changes that occur as they have the full and complete context of the resource and the changes made to it. Special Note: \u00b6 DynamoDB Streams in Global DynamoDB tables invoke this Lambda whenever a DynamoDB update occurs in ANY of the regions the table is configured to sync with. For the Current table, this can result in Historical Lambda functions \"stepping on each other's toes\" (this is not a concern for Durable table changes). To avoid this, the Current table DynamoDB Stream Proxy has a PROXY_REGIONS environment variable that is configured to only proxy DynamoDB Stream updates that occur to resources that reside in the specified regions. The PRIMARY REGION must be configured to proxy events that occur in the primary region, and all off-regions. The SECONDARY REGION(S) must be configured to proxy events that occur in the same region. Another Special Note: \u00b6 DynamoDB items are capped to 400KB. SNS and SQS have maximum message sizes of 256KB. Logic exists to handle cases where DynamoDB items are too big to send over to SNS/SQS. Follow-up Lambdas and subscribers will need to make use of the Historical code to fetch the full configuration of the item either out of the Current or Durable tables (depending on the use case). Enhancements will be made in the future to help address this to make the data easier to consume in these (rare) circumstances. Differ \u00b6 The Differ is a Lambda function that gets invoked upon changes to the Current table. The DynamoDB stream provides the Differ (via the Proxy) the current state of the resource that changed. The Differ checks if the resource in question has had an effective change. If so, the Differ saves a new change record to the Durable table to maintain history of the resource as it changes over time, and also saves the CloudTrail context. Durable Table \u00b6 The \"Durable\" table is a Global DynamoDB table that stores a resource configuration with change history. The Durable table has as DynamoDB Stream that invokes another DynamoDB Stream Proxy. This is used to notify downstream subscribers of the effective changes that occur to the environment. Off-Region SNS Forwarders \u00b6 Very bare infrastructure is intentionally deployed in the off-regions. This helps to reduce costs and complexity of the Historical infrastructure. The off-region SNS forwarders are SNS topics that receive CloudWatch events for resource changes that occur in the off-regions. These topics forward events to the Event Collector SQS queue in the primary region for processing. Special Stacks \u00b6 Some resource types have different stack configurations due to nuances of the resource type. The following resource types have different stack types: S3 IAM (Coming Soon!) S3 \u00b6 The AWS S3 stack is almost identical to the standard stack. The difference is due to AWS S3 buckets having a globally unique namespace. For S3, because it is not presently possible to only poll for in-region S3 buckets, the poller lives in the primary region only. The poller in the primary region polls for all S3 buckets in all regions. The secondary regions will still respond to in-region events, but lack all polling components. This diagram showcases the S3 stack. IAM \u00b6 This is coming soon! Installation & Configuration \u00b6 Please refer to the installation docs for additional details.","title":"Architecture"},{"location":"architecture/#historical-architecture","text":"Historical is a serverless AWS application that consists of many components. Historical is written in Python 3 and heavily leverages AWS technologies such as Lambda, SNS, SQS, DynamoDB, CloudTrail, and CloudWatch.","title":"Historical Architecture"},{"location":"architecture/#general-architectural-overview","text":"Here is a diagram of the Historical Architecture: Please Note: This stack is deployed for every technology monitored ! There are many, many Historical stacks that will be deployed.","title":"General Architectural Overview"},{"location":"architecture/#polling-vs-events","text":"Historical is both a polling and event driven system. It will periodically poll AWS accounts for changes. However, because Historical responds to events in the environment, polling doesn't need to be very aggressive and only happens once every few hours. Polling is necessary because events are not 100% reliable. This ensures that data is current just in case an event is dropped. Historical is eventually consistent , and makes a best effort to maintain a current and up-to-date inventory of AWS resources.","title":"Polling vs. Events"},{"location":"architecture/#prerequisite-overview","text":"This is a high-level overview of the prerequisites that are required to make Historical operate. For more details on setting up the required prerequisites, please review the installation documentation . ALL AWS accounts accounts have CloudTrail enabled. ALL AWS accounts and ALL regions in those accounts have a CloudWatch Event rule that captures ALL events and sends them over the CloudWatch Event Bus to the Historical account for processing. IAM roles exist in ALL accounts and are assumable by the Historical Lambda functions. Historical makes use of SWAG to define which AWS accounts Historical is enabled for. While not a hard requirement, use of SWAG is highly recommended .","title":"Prerequisite Overview"},{"location":"architecture/#regions","text":"Historical has the concept of regions that fit 3 categories: Primary region Secondary region(s) Off region(s) The Primary Region is considered the \"Base\" of Historical. This region has all of the major components that make up Historical. This region processes all in-region AND off-region originating events. The Off Region(s) are regions you don't have a lot of infrastructure deployed in. However, you still want visibility in these regions should events happen there. These regions have very minimal amount of Historical-related infrastructure deployed. These regions will forward ALL events to the Primary Region for processing. The Secondary Region(s) are regions that are important to you. Secondary regions look like the primary region and process in-region events. If you have a lot of infrastructure within a region, you should place a Historical stack there. This will allow you to quickly receive and process events, and also gives your applications a regionally-local means of accessing Historical data. Note: Place a Historical off-region stack in any region that is not Primary or Secondary. This will ensure full visibility in your environment.","title":"Regions"},{"location":"architecture/#component-overview","text":"This section describes some of the high-level architectural components.","title":"Component Overview"},{"location":"architecture/#primary-components","text":"Below are the primary components of the Historical architecture: CloudWatch Event Rules CloudWatch Change Events Poller Collector Current Table DynamoDB Stream Proxy Differ Durable Table Off-region SNS forwarders As general overview, the infrastructure is an event processing and enriching pipeline. An event will arrive, will get enriched with additional information, and will provide notifications to downstream subscribers on the given changes. SQS queues are used in as many places as much as possible to invoke Lambda functions. SQS makes it easy to provide Lambda execution concurrency, auto-scaling, retry of failures without blocking, and dead-letter queuing capabilities. SNS topics are used to make it easy for N number of interested parties to subscribe to the Historical DynamoDB table changes. Presently, this is only attached to the Durable table. More details on this below.","title":"Primary Components"},{"location":"architecture/#cloudwatch-event-rules","text":"There are two different CloudWatch Event Rules: Timed Events Change Events Timed events are used to kick off the Poller. See the section on the poller below for additional details. Change events are events that arrive from CloudWatch Events when an AWS resource's configuration changes.","title":"CloudWatch Event Rules"},{"location":"architecture/#poller","text":"The Poller's primary function is to obtain a full inventory of AWS resources. The Poller is split into two parts: Poller Tasker Poller The \"Poller Tasker\" is a Lambda function that iterates over all AWS accounts Historical is configured for, and tasks the Poller to list all resources in the given environment. The Poller Tasker in the PRIMARY REGION tasks the Poller to list resources that reside in the primary region and all off-regions. A Poller Tasker in a SECONDARY REGION will only task a poller to describe resources that reside in the same region. The Poller lists all resources in a given account/region, and tasks a \"Poller Collector\" to fetch details about the resource in question.","title":"Poller"},{"location":"architecture/#collector","text":"The Collector describes a given AWS resource and stores its configuration to the \"Current\" DynamoDB table. The Collector is split into two parts (same code, different invocation mechanisms): Poller Collector Event Collector The Poller Collector is a collector that will only respond to polling events. The Event Collector will only respond to CloudWatch change events. The Collector is split into two parts to prevent change events from being sandwiched in between polling events. Historical gives priority to change events over polling events to ensure timeliness of resource configuration changes. In both cases, the Collector will go to the AWS account and region that the item resides in, and use boto3 to describe the configuration of the resource.","title":"Collector"},{"location":"architecture/#current-table","text":"The \"Current\" table is a global DynamoDB table that stores the current configuration of a given resource in AWS. This acts as a cache for current the state of the environment. The Current table has as DynamoDB Stream that will kick off a DynamoDB Stream Proxy that then invokes the Differ.","title":"Current Table"},{"location":"architecture/#special-note","text":"The Current table has a TTL set on all items. This TTL is updated any time a change event arrives, or when the Poller runs. The TTL is set to clean-up orphaned items, which can happen if a deletion event is lost. Deleted items will not be picked up by the Poller (only lists items that exist in the account) and thus, will be removed from the Current table on TTL expiration. As a result, the Poller must \"see\" a resource at least once every few hours before it is deemed deleted from the environment.","title":"Special Note:"},{"location":"architecture/#dynamodb-stream-proxy","text":"The DynamoDB Stream Proxy is a Lambda function that proxies DynamoDB Stream events to SNS or SQS. The purpose is to task subsequent Lambda functions on the specific changes that happen to the DynamoDB table. The Historical infrastructure has two configurations for the DynamoDB Proxy: Current Table Forwarder (DynamoDB Stream Proxy to Differ SQS) Durable Table Forwarder (DynamoDB Stream Proxy to Change Notification SNS) The Current Table Forwarder proxies events to the SQS queue that invokes the Differ Lambda function. The Durable Table Forwarder proxies events to an SNS topic that can be subscribed. SNS enables N subscribers to Historical events. The Durable table proxy serializes the DynamoDB Stream events into an easily consumable JSON that contains the full and complete configuration of the resource in question, along with the the CloudTrail context. This enables downstream applications to make intelligent decisions about the changes that occur as they have the full and complete context of the resource and the changes made to it.","title":"DynamoDB Stream Proxy"},{"location":"architecture/#special-note_1","text":"DynamoDB Streams in Global DynamoDB tables invoke this Lambda whenever a DynamoDB update occurs in ANY of the regions the table is configured to sync with. For the Current table, this can result in Historical Lambda functions \"stepping on each other's toes\" (this is not a concern for Durable table changes). To avoid this, the Current table DynamoDB Stream Proxy has a PROXY_REGIONS environment variable that is configured to only proxy DynamoDB Stream updates that occur to resources that reside in the specified regions. The PRIMARY REGION must be configured to proxy events that occur in the primary region, and all off-regions. The SECONDARY REGION(S) must be configured to proxy events that occur in the same region.","title":"Special Note:"},{"location":"architecture/#another-special-note","text":"DynamoDB items are capped to 400KB. SNS and SQS have maximum message sizes of 256KB. Logic exists to handle cases where DynamoDB items are too big to send over to SNS/SQS. Follow-up Lambdas and subscribers will need to make use of the Historical code to fetch the full configuration of the item either out of the Current or Durable tables (depending on the use case). Enhancements will be made in the future to help address this to make the data easier to consume in these (rare) circumstances.","title":"Another Special Note:"},{"location":"architecture/#differ","text":"The Differ is a Lambda function that gets invoked upon changes to the Current table. The DynamoDB stream provides the Differ (via the Proxy) the current state of the resource that changed. The Differ checks if the resource in question has had an effective change. If so, the Differ saves a new change record to the Durable table to maintain history of the resource as it changes over time, and also saves the CloudTrail context.","title":"Differ"},{"location":"architecture/#durable-table","text":"The \"Durable\" table is a Global DynamoDB table that stores a resource configuration with change history. The Durable table has as DynamoDB Stream that invokes another DynamoDB Stream Proxy. This is used to notify downstream subscribers of the effective changes that occur to the environment.","title":"Durable Table"},{"location":"architecture/#off-region-sns-forwarders","text":"Very bare infrastructure is intentionally deployed in the off-regions. This helps to reduce costs and complexity of the Historical infrastructure. The off-region SNS forwarders are SNS topics that receive CloudWatch events for resource changes that occur in the off-regions. These topics forward events to the Event Collector SQS queue in the primary region for processing.","title":"Off-Region SNS Forwarders"},{"location":"architecture/#special-stacks","text":"Some resource types have different stack configurations due to nuances of the resource type. The following resource types have different stack types: S3 IAM (Coming Soon!)","title":"Special Stacks"},{"location":"architecture/#s3","text":"The AWS S3 stack is almost identical to the standard stack. The difference is due to AWS S3 buckets having a globally unique namespace. For S3, because it is not presently possible to only poll for in-region S3 buckets, the poller lives in the primary region only. The poller in the primary region polls for all S3 buckets in all regions. The secondary regions will still respond to in-region events, but lack all polling components. This diagram showcases the S3 stack.","title":"S3"},{"location":"architecture/#iam","text":"This is coming soon!","title":"IAM"},{"location":"architecture/#installation-configuration","text":"Please refer to the installation docs for additional details.","title":"Installation &amp; Configuration"},{"location":"troubleshooting/","text":"Troubleshooting \u00b6 This doc will be updated in the future.","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"This doc will be updated in the future.","title":"Troubleshooting"},{"location":"installation/","text":"Installation & Configuration \u00b6 Note: Some assembly is required. There are many components that make up Historical. Included is a Docker container that you can use to run Terraform for installation. Please review each section below in order to ensure that all aspects of the installation go smoothly. This is important because there are many components that have to be configured correctly for Historical to operate properly. Architecture \u00b6 Before reading this installation guide, please become familiar with the Historical architecture. This will assist you in making the proper configuration for Historical. You can review that here . Prerequisites \u00b6 Historical requires the following prerequisites: An AWS account that is dedicated for Historical (this is highly recommended). CloudTrail must be enabled for ALL accounts and ALL regions. CloudWatch Event Buses must be configured to route ALL CloudWatch Events to the Historical account. Please review and follow the AWS documentation for sending and receiving events between AWS accounts before continuing . This diagram outlines how CloudWatch Event Buses should be configured: You will need to create IAM roles in all the accounts to monitor first. This requires your own orchestration to complete. See the IAM section below for details. Historical makes use of SWAG to define which AWS accounts Historical is enabled for. SWAG must be properly configured for Historical to operate. Alternatively, you can specify the AWS Account IDs that Historical will examine via an environment variable. However, it is highly recommended that you make use of SWAG. IAM Setup \u00b6 Please review the IAM Role setup guide here for instructions. Terraform \u00b6 A set of sample Terraform templates are included to assist with the roll-out of the infrastructure. This is intended to be run within a Docker container (code also included). The Docker container will: This is used for both installation and uninstallation. Please review the documentation in detail here . Configuration and Environment Variables \u00b6 IMPORTANT: There are many environment variables and configuration details that are required to be set. Please review this page for details on this . Prepare Docker Container \u00b6 Once you have made the necessary changes to your Terraform configuration files, you need to build the Docker container. You will need to build your Docker container. Please install Docker if you haven't already. Navigate to the historical/terraform directory. In a terminal, run docker build . -t historical_installer At this point, you now have a Docker container with all the required components to deploy Historical. If you need to make any adjustments, you will need to re-build your container. Installation \u00b6 Terraform requires a lot of permissions. You will need a very powerful AWS administrative role with lots of permissions to execute the Docker. Get credentials from an IAM role with administrative permissions. Make a copy of terraform/SAMPLE-env.list to terraform/env.list Open terraform/env.list , and fill in the values. ALL values must be supplied and correct. See the configuration documentation for reference. In a terminal, navigate to terraform/ Run Docker! docker run --env-file ./env.list -t historical_installer Hopefully this works! Uninstallation \u00b6 Like for installation, you will need a lot of permissions. You will need a very powerful AWS administrative role with lots of permissions to execute the Docker. Get credentials from an IAM role with administrative permissions. Use the terraform/env.list values used for installation. In a terminal, navigate to terraform/ Run Docker! docker run --env-file ./env.list --entrypoint /installer/teardown_historical.sh -t historical_installer This might fail the first time it runs. This is because Terraform doesn't wait long enough for all the resources to be deleted in the primary region. Try running it again if it fails the first time. If it's still failing, you may need to find the resources that are failing to delete and manually delete them. Please note: Depending on how active the Lambda functions are, the CloudWatch Event Log groups may still be present after stack deletion. You will need to manually delete these in each primary and secondary regions. Hopefully this works well for you! Troubleshooting \u00b6 Please review the Troubleshooting doc if you are experiencing issues.","title":"Installation & Configuration"},{"location":"installation/#installation-configuration","text":"Note: Some assembly is required. There are many components that make up Historical. Included is a Docker container that you can use to run Terraform for installation. Please review each section below in order to ensure that all aspects of the installation go smoothly. This is important because there are many components that have to be configured correctly for Historical to operate properly.","title":"Installation &amp; Configuration"},{"location":"installation/#architecture","text":"Before reading this installation guide, please become familiar with the Historical architecture. This will assist you in making the proper configuration for Historical. You can review that here .","title":"Architecture"},{"location":"installation/#prerequisites","text":"Historical requires the following prerequisites: An AWS account that is dedicated for Historical (this is highly recommended). CloudTrail must be enabled for ALL accounts and ALL regions. CloudWatch Event Buses must be configured to route ALL CloudWatch Events to the Historical account. Please review and follow the AWS documentation for sending and receiving events between AWS accounts before continuing . This diagram outlines how CloudWatch Event Buses should be configured: You will need to create IAM roles in all the accounts to monitor first. This requires your own orchestration to complete. See the IAM section below for details. Historical makes use of SWAG to define which AWS accounts Historical is enabled for. SWAG must be properly configured for Historical to operate. Alternatively, you can specify the AWS Account IDs that Historical will examine via an environment variable. However, it is highly recommended that you make use of SWAG.","title":"Prerequisites"},{"location":"installation/#iam-setup","text":"Please review the IAM Role setup guide here for instructions.","title":"IAM Setup"},{"location":"installation/#terraform","text":"A set of sample Terraform templates are included to assist with the roll-out of the infrastructure. This is intended to be run within a Docker container (code also included). The Docker container will: This is used for both installation and uninstallation. Please review the documentation in detail here .","title":"Terraform"},{"location":"installation/#configuration-and-environment-variables","text":"IMPORTANT: There are many environment variables and configuration details that are required to be set. Please review this page for details on this .","title":"Configuration and Environment Variables"},{"location":"installation/#prepare-docker-container","text":"Once you have made the necessary changes to your Terraform configuration files, you need to build the Docker container. You will need to build your Docker container. Please install Docker if you haven't already. Navigate to the historical/terraform directory. In a terminal, run docker build . -t historical_installer At this point, you now have a Docker container with all the required components to deploy Historical. If you need to make any adjustments, you will need to re-build your container.","title":"Prepare Docker Container"},{"location":"installation/#installation","text":"Terraform requires a lot of permissions. You will need a very powerful AWS administrative role with lots of permissions to execute the Docker. Get credentials from an IAM role with administrative permissions. Make a copy of terraform/SAMPLE-env.list to terraform/env.list Open terraform/env.list , and fill in the values. ALL values must be supplied and correct. See the configuration documentation for reference. In a terminal, navigate to terraform/ Run Docker! docker run --env-file ./env.list -t historical_installer Hopefully this works!","title":"Installation"},{"location":"installation/#uninstallation","text":"Like for installation, you will need a lot of permissions. You will need a very powerful AWS administrative role with lots of permissions to execute the Docker. Get credentials from an IAM role with administrative permissions. Use the terraform/env.list values used for installation. In a terminal, navigate to terraform/ Run Docker! docker run --env-file ./env.list --entrypoint /installer/teardown_historical.sh -t historical_installer This might fail the first time it runs. This is because Terraform doesn't wait long enough for all the resources to be deleted in the primary region. Try running it again if it fails the first time. If it's still failing, you may need to find the resources that are failing to delete and manually delete them. Please note: Depending on how active the Lambda functions are, the CloudWatch Event Log groups may still be present after stack deletion. You will need to manually delete these in each primary and secondary regions. Hopefully this works well for you!","title":"Uninstallation"},{"location":"installation/#troubleshooting","text":"Please review the Troubleshooting doc if you are experiencing issues.","title":"Troubleshooting"},{"location":"installation/configuration/","text":"Historical Environment Variables & Configuration \u00b6 Below is a reference of all of the environment variables that Historical makes use of, and the required/default status of them: Most of these variables are found in: historical/constants.py historical/mapping/__init__.py NOTE: All environment variables are Strings Required Fields \u00b6 The fields below are required and MUST be configured by you in your Terraform templates: Variable Where to set Sample Value PRIMARY_REGION Per-stack Terraform template variable PRIMARY_REGION us-west-2 POLLING_REGIONS Per-stack Terraform template variable POLLING_REGIONS [\"us-west-2\", \"us-east-1\", \"eu-west-1\"] This should be set to the secondary regions for most stacks. S3 is the exception since it's a \"global\" namespace. For S3, this is always set to the PRIMARY_REGION . This populates the POLL_REGIONS env. var for the Poller Lambdas. REGION Infrastructure main.tf This is a variable supplied to Terraform in the application of the template. This value is used to determine if the current region of the deployment is the primary region or a secondary region. PROXY_REGIONS Per-stack Terraform template current_proxy_env_vars and durable_proxy_env_vars us-east-1,eu-west-1,us-east-2,etc. This is a comma-separated string of regions. The current_proxy_env_vars for the PRIMARY_REGION needs to be configured to contain the PRIMARY_REGION and all the \"off-regions\". The durable_proxy_env_vars should contain ALL the regions (default). HISTORICAL_TECHNOLOGY Per-stack Terraform template durable_proxy_env_vars s3 or securitygroup . This should be set in each sample stack properly. SIMPLE_DURABLE_PROXY Per-stack Terraform template durable_proxy_env_vars True - This is the default value for the Durable Proxy. Don't change this. This value toggles whether the DynamoDB stream events will be serialized nicely for downstream consumption or not. ENABLED_ACCOUNTS Per-stack Terraform template env_vars ACCOUNTID1,ACCOUNTID2,etc. If you are not making use of SWAG , then you need to set this. SWAG_BUCKET Per-stack Terraform template env_vars some-s3-bucket-name Required if you are making use of SWAG . SWAG_DATA_FILE Per-stack Terraform template env_vars v2/accounts.json Required if you are making use of SWAG . Points to where the accounts.json file is located. SWAG_OWNER Per-stack Terraform template env_vars yourcompany Required if you are making use of SWAG . The entity that owns the accounts you are monitoring. SWAG_REGION Per-stack Terraform template env_vars us-west-2 Required if you are making use of SWAG . The region the SWAG_BUCKET is located. Default Required Fields \u00b6 These are fields that are required, but the default values are sufficient. These are not set in the Terraform templates. Variable Description & Defaults CURRENT_REGION This is populated by the AWS_DEFAULT_REGION environment variable provided by Lambda. This will be set to the region that the Lambda function is running in. TTL_EXPIRY Default: 86400 seconds. This is the TTL for an item in the Current Table. This is used to account for missing deletion events. HISTORICAL_ROLE Default: Historical . Don't change this -- this is the name of the IAM role that Historical needs to assume to describe resources. REGION_ATTR Default: Region . Don't change this -- this is the name of the region attribute in the DynamoDB table. EVENT_TOO_BIG_FLAG Default: event_too_big . Don't change this -- this is a field name that informs Historical downstream functions if an event is too big to fit in SNS and SQS (>256KB). Optional Fields \u00b6 Variable Where to set Sample Value RANDOMIZE_POLLER Per-stack Terraform template poller_env_vars 0 <= value <= 900. Number of seconds to delay Polling messages in SQS. It is recommended you set this to \"900\" for the Poller. LOGGING_LEVEL Per-stack Terraform template env_vars Any one of these values . DEBUG is recommended. TEST_ACCOUNTS_ONLY Per-stack Terraform template env_vars Default False . This is used if you are making use of SWAG . Set this to True if you want your stack to ONLY query against \"test\" accounts. Useful for having \"test\" and \"prod\" stacks. PROXY_BATCH_SIZE Per-stack Terraform template current_proxy_env_vars . Default: 10 . Set this if the batched event size is too big (>256KB) to send to SQS. This should be refactored in the future so that this is not necessary. SENTRY_DSN Per-stack Terraform template env_vars If you make use of Sentry , then set this to your DSN. Historical makes use of the raven-python-lambda for Sentry. You can also optionally use SQS as a transport layer for Sentry messages via raven-sqs-proxy . Custom Tags Per-stack Terraform template tags Add in a name-value pair of tags you want to affix to your Lambda functions. Docker Installer Specific Fields \u00b6 The fields below are specific for installation and uninstallation of Historical via the Docker container. These values are present in the terraform/SAMPLE-env.list file. ALL FIELDS BELOW ARE REQUIRED Variable Sample Value AWS_ACCESS_KEY_ID The AWS Access Key ID for the credential that will be used to run Terraform. This is for a very powerful IAM Role. AWS_SECRET_ACCESS_KEY The AWS Secret Access Key for the credential that will be used to run Terraform. This is for a very powerful IAM Role. AWS_SESSION_TOKEN The AWS Session Token for the credential that will be used to run Terraform. This is for a very powerful IAM Role. TECH The Historical resource type for the stack in question. Either s3 or securitygroup (for now). PRIMARY_REGION The Primary Region of your Historical Stack. SECONDARY_REGIONS The Secondary Regions of your Historical Stack. This is a comma separated string. Next Steps \u00b6 Please return to the Installation documentation .","title":"Configuration Reference"},{"location":"installation/configuration/#historical-environment-variables-configuration","text":"Below is a reference of all of the environment variables that Historical makes use of, and the required/default status of them: Most of these variables are found in: historical/constants.py historical/mapping/__init__.py NOTE: All environment variables are Strings","title":"Historical Environment Variables &amp; Configuration"},{"location":"installation/configuration/#required-fields","text":"The fields below are required and MUST be configured by you in your Terraform templates: Variable Where to set Sample Value PRIMARY_REGION Per-stack Terraform template variable PRIMARY_REGION us-west-2 POLLING_REGIONS Per-stack Terraform template variable POLLING_REGIONS [\"us-west-2\", \"us-east-1\", \"eu-west-1\"] This should be set to the secondary regions for most stacks. S3 is the exception since it's a \"global\" namespace. For S3, this is always set to the PRIMARY_REGION . This populates the POLL_REGIONS env. var for the Poller Lambdas. REGION Infrastructure main.tf This is a variable supplied to Terraform in the application of the template. This value is used to determine if the current region of the deployment is the primary region or a secondary region. PROXY_REGIONS Per-stack Terraform template current_proxy_env_vars and durable_proxy_env_vars us-east-1,eu-west-1,us-east-2,etc. This is a comma-separated string of regions. The current_proxy_env_vars for the PRIMARY_REGION needs to be configured to contain the PRIMARY_REGION and all the \"off-regions\". The durable_proxy_env_vars should contain ALL the regions (default). HISTORICAL_TECHNOLOGY Per-stack Terraform template durable_proxy_env_vars s3 or securitygroup . This should be set in each sample stack properly. SIMPLE_DURABLE_PROXY Per-stack Terraform template durable_proxy_env_vars True - This is the default value for the Durable Proxy. Don't change this. This value toggles whether the DynamoDB stream events will be serialized nicely for downstream consumption or not. ENABLED_ACCOUNTS Per-stack Terraform template env_vars ACCOUNTID1,ACCOUNTID2,etc. If you are not making use of SWAG , then you need to set this. SWAG_BUCKET Per-stack Terraform template env_vars some-s3-bucket-name Required if you are making use of SWAG . SWAG_DATA_FILE Per-stack Terraform template env_vars v2/accounts.json Required if you are making use of SWAG . Points to where the accounts.json file is located. SWAG_OWNER Per-stack Terraform template env_vars yourcompany Required if you are making use of SWAG . The entity that owns the accounts you are monitoring. SWAG_REGION Per-stack Terraform template env_vars us-west-2 Required if you are making use of SWAG . The region the SWAG_BUCKET is located.","title":"Required Fields"},{"location":"installation/configuration/#default-required-fields","text":"These are fields that are required, but the default values are sufficient. These are not set in the Terraform templates. Variable Description & Defaults CURRENT_REGION This is populated by the AWS_DEFAULT_REGION environment variable provided by Lambda. This will be set to the region that the Lambda function is running in. TTL_EXPIRY Default: 86400 seconds. This is the TTL for an item in the Current Table. This is used to account for missing deletion events. HISTORICAL_ROLE Default: Historical . Don't change this -- this is the name of the IAM role that Historical needs to assume to describe resources. REGION_ATTR Default: Region . Don't change this -- this is the name of the region attribute in the DynamoDB table. EVENT_TOO_BIG_FLAG Default: event_too_big . Don't change this -- this is a field name that informs Historical downstream functions if an event is too big to fit in SNS and SQS (>256KB).","title":"Default Required Fields"},{"location":"installation/configuration/#optional-fields","text":"Variable Where to set Sample Value RANDOMIZE_POLLER Per-stack Terraform template poller_env_vars 0 <= value <= 900. Number of seconds to delay Polling messages in SQS. It is recommended you set this to \"900\" for the Poller. LOGGING_LEVEL Per-stack Terraform template env_vars Any one of these values . DEBUG is recommended. TEST_ACCOUNTS_ONLY Per-stack Terraform template env_vars Default False . This is used if you are making use of SWAG . Set this to True if you want your stack to ONLY query against \"test\" accounts. Useful for having \"test\" and \"prod\" stacks. PROXY_BATCH_SIZE Per-stack Terraform template current_proxy_env_vars . Default: 10 . Set this if the batched event size is too big (>256KB) to send to SQS. This should be refactored in the future so that this is not necessary. SENTRY_DSN Per-stack Terraform template env_vars If you make use of Sentry , then set this to your DSN. Historical makes use of the raven-python-lambda for Sentry. You can also optionally use SQS as a transport layer for Sentry messages via raven-sqs-proxy . Custom Tags Per-stack Terraform template tags Add in a name-value pair of tags you want to affix to your Lambda functions.","title":"Optional Fields"},{"location":"installation/configuration/#docker-installer-specific-fields","text":"The fields below are specific for installation and uninstallation of Historical via the Docker container. These values are present in the terraform/SAMPLE-env.list file. ALL FIELDS BELOW ARE REQUIRED Variable Sample Value AWS_ACCESS_KEY_ID The AWS Access Key ID for the credential that will be used to run Terraform. This is for a very powerful IAM Role. AWS_SECRET_ACCESS_KEY The AWS Secret Access Key for the credential that will be used to run Terraform. This is for a very powerful IAM Role. AWS_SESSION_TOKEN The AWS Session Token for the credential that will be used to run Terraform. This is for a very powerful IAM Role. TECH The Historical resource type for the stack in question. Either s3 or securitygroup (for now). PRIMARY_REGION The Primary Region of your Historical Stack. SECONDARY_REGIONS The Secondary Regions of your Historical Stack. This is a comma separated string.","title":"Docker Installer Specific Fields"},{"location":"installation/configuration/#next-steps","text":"Please return to the Installation documentation .","title":"Next Steps"},{"location":"installation/iam/","text":"Historical IAM Role Setup Guide \u00b6 IAM roles need to be configured for Historical to properly inventory all of your accounts. The following must be created: The HistoricalLambdaProfile role which is used to launch the Historical Lambda functions. The Historical role which the HistoricalLambdaProfile will assume to describe and collect details from the account in question. The architecture for this looks like this: Instructions \u00b6 Lambda Role \u00b6 In the Historical account, create the HistoricalLambdaProfile IAM Role. This role needs to permit the lambda.amazonaws.com Service Principal access to it. Here is an example: Trust Policy : { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"lambda.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" } ] } This role is being executed by AWS Lambda and requires the AWSLambdaBasicExecutionRole AWS managed policy attached to it. This managed policy gives the Lambda access to write to CloudWatch Logs. VPC permissions are not required because Historical does not make use of ENIs or Security Groups. The role then needs a set of Inline Policies to grant it access to the resources required for the Lambda function to access the Historical resources. Please make a new Inline Policy named HistoricalLambdaPerms as follows (substitute HISTORICAL-ACCOUNT-NUMBER-HERE with the AWS account ID of the Historical account): { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"SQS\", \"Effect\": \"Allow\", \"Action\": [ \"sqs:DeleteMessage\", \"sqs:GetQueueAttributes\", \"sqs:GetQueueUrl\", \"sqs:ReceiveMessage\", \"sqs:SendMessage\" ], \"Resource\": \"arn:aws:sqs:*:HISTORICAL-ACCOUNT-NUMBER-HERE:Historical*\" }, { \"Sid\": \"SNS\", \"Effect\": \"Allow\", \"Action\": \"sns:Publish\", \"Resource\": \"arn:aws:sns:*:HISTORICAL-ACCOUNT-NUMBER-HERE:Historical*\" }, { \"Sid\": \"STS\", \"Effect\": \"Allow\", \"Action\": \"sts:AssumeRole\", \"Resource\": \"arn:aws:iam::*:role/Historical\" }, { \"Sid\": \"DynamoDB\", \"Effect\": \"Allow\", \"Action\": [ \"dynamodb:BatchGetItem\", \"dynamodb:BatchWriteItem\", \"dynamodb:DeleteItem\", \"dynamodb:DescribeStream\", \"dynamodb:DescribeTable\", \"dynamodb:GetItem\", \"dynamodb:GetRecords\", \"dynamodb:GetShardIterator\", \"dynamodb:ListStreams\", \"dynamodb:PutItem\", \"dynamodb:Query\", \"dynamodb:Scan\", \"dynamodb:UpdateItem\" ], \"Resource\": \"arn:aws:dynamodb:*:HISTORICAL-ACCOUNT-NUMBER-HERE:table/Historical*\" } ] } Destination Account Roles \u00b6 You will mostly likely need your own orchestration to roll this out. This will need to be rolled out to ALL accounts that you are inventorying with Historical. The role is named Historical and has the following configuration details: Trust Policy (substitute HISTORICAL-ACCOUNT-NUMBER-HERE with the AWS account ID of the Historical account): { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::HISTORICAL-ACCOUNT-NUMBER-HERE:role/HistoricalLambdaProfile\" }, \"Action\": \"sts:AssumeRole\", \"Condition\": {} } ] } The Historical role needs read access to your resources. Simply attach the ReadOnlyAccess AWS managed policy to the role and that is all. Duplicate this role to all of your accounts via your own orchestration and automation. Next Steps \u00b6 Please return to the Installation documentation .","title":"IAM Setup"},{"location":"installation/iam/#historical-iam-role-setup-guide","text":"IAM roles need to be configured for Historical to properly inventory all of your accounts. The following must be created: The HistoricalLambdaProfile role which is used to launch the Historical Lambda functions. The Historical role which the HistoricalLambdaProfile will assume to describe and collect details from the account in question. The architecture for this looks like this:","title":"Historical IAM Role Setup Guide"},{"location":"installation/iam/#instructions","text":"","title":"Instructions"},{"location":"installation/iam/#lambda-role","text":"In the Historical account, create the HistoricalLambdaProfile IAM Role. This role needs to permit the lambda.amazonaws.com Service Principal access to it. Here is an example: Trust Policy : { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"lambda.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" } ] } This role is being executed by AWS Lambda and requires the AWSLambdaBasicExecutionRole AWS managed policy attached to it. This managed policy gives the Lambda access to write to CloudWatch Logs. VPC permissions are not required because Historical does not make use of ENIs or Security Groups. The role then needs a set of Inline Policies to grant it access to the resources required for the Lambda function to access the Historical resources. Please make a new Inline Policy named HistoricalLambdaPerms as follows (substitute HISTORICAL-ACCOUNT-NUMBER-HERE with the AWS account ID of the Historical account): { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"SQS\", \"Effect\": \"Allow\", \"Action\": [ \"sqs:DeleteMessage\", \"sqs:GetQueueAttributes\", \"sqs:GetQueueUrl\", \"sqs:ReceiveMessage\", \"sqs:SendMessage\" ], \"Resource\": \"arn:aws:sqs:*:HISTORICAL-ACCOUNT-NUMBER-HERE:Historical*\" }, { \"Sid\": \"SNS\", \"Effect\": \"Allow\", \"Action\": \"sns:Publish\", \"Resource\": \"arn:aws:sns:*:HISTORICAL-ACCOUNT-NUMBER-HERE:Historical*\" }, { \"Sid\": \"STS\", \"Effect\": \"Allow\", \"Action\": \"sts:AssumeRole\", \"Resource\": \"arn:aws:iam::*:role/Historical\" }, { \"Sid\": \"DynamoDB\", \"Effect\": \"Allow\", \"Action\": [ \"dynamodb:BatchGetItem\", \"dynamodb:BatchWriteItem\", \"dynamodb:DeleteItem\", \"dynamodb:DescribeStream\", \"dynamodb:DescribeTable\", \"dynamodb:GetItem\", \"dynamodb:GetRecords\", \"dynamodb:GetShardIterator\", \"dynamodb:ListStreams\", \"dynamodb:PutItem\", \"dynamodb:Query\", \"dynamodb:Scan\", \"dynamodb:UpdateItem\" ], \"Resource\": \"arn:aws:dynamodb:*:HISTORICAL-ACCOUNT-NUMBER-HERE:table/Historical*\" } ] }","title":"Lambda Role"},{"location":"installation/iam/#destination-account-roles","text":"You will mostly likely need your own orchestration to roll this out. This will need to be rolled out to ALL accounts that you are inventorying with Historical. The role is named Historical and has the following configuration details: Trust Policy (substitute HISTORICAL-ACCOUNT-NUMBER-HERE with the AWS account ID of the Historical account): { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::HISTORICAL-ACCOUNT-NUMBER-HERE:role/HistoricalLambdaProfile\" }, \"Action\": \"sts:AssumeRole\", \"Condition\": {} } ] } The Historical role needs read access to your resources. Simply attach the ReadOnlyAccess AWS managed policy to the role and that is all. Duplicate this role to all of your accounts via your own orchestration and automation.","title":"Destination Account Roles"},{"location":"installation/iam/#next-steps","text":"Please return to the Installation documentation .","title":"Next Steps"},{"location":"installation/terraform/","text":"Historical Terraform Setup \u00b6 A set of sample Terraform templates are included to assist with the roll-out of the infrastructure. This is intended to be run within a Docker container (code also included). The Docker container will: Package the Historical Lambda code Run the Terraform templates to provision all of the infrastructure This is all run within an Amazon Linux Docker container. Amazon Linux is required because Historical's dependencies make use of statically linked libraries, which will fail to run in the Lambda environment unless the binaries are built on Amazon Linux. You can also use this to uninstall Historical from your environment as well. Please review each section below, as the details are very important: Structure \u00b6 The Terraform templates are split into multiple components: Terraform Plugins (located in terraform/terraform-plugins) DynamoDB (located in terraform/dynamodb) Infrastructure (located in terraform/infra) Terraform Backend Configuration \u00b6 We make the assumption that the Terraform backend is on S3. As such, you will need an S3 bucket that resides in the Historical AWS account. It is highly recommended that you configure the Historical Terraform S3 bucket with versioning enabled. This is needed should there ever be an issue with the Terraform state. NOTE: For ALL Terraform main.tf template files, at the top of the template file is a backend region configuration. It looks like this: terraform { backend \"s3\" { // Set this to where your Terraform S3 bucket is located (using us-west-2 as the example): region = \"us-west-2\" } } You will need to set the region to where your Terraform S3 bucket resides. In our examples, we are making use of us-west-2 . Terraform Plugins \u00b6 This is a Terraform template that is executed in the Docker build step. This is done to pin the Terraform plugins to the Docker container so that they need not be re-downloaded later. It is important to keep the version numbers in this doc in sync with the rest of the templates. DynamoDB Templates \u00b6 This is used to construct the Global DynamoDB tables used by Historical. This is structured as follows: main.tf - This is the main template with the components required to build out the Global DynamoDB tables for a given Historical stack. The sample included makes an ASSUMPTION that you will be utilizing us-west-2 as your PRIMARY REGION , and us-east-1 and eu-west-1 as your SECONDARY REGIONS . You will need to modify this template accordingly to change the defaults set. This is used for ALL stacks. If you want to specify different primary and secondary regions for a given AWS resource type, then you will need to make your own modifications to the installation scripting to leverage different templates. Per-resource type stack configurations. Included are details for S3 and Security Groups. There is a Terraform template for each resource type. This is where you can configure the read and write capacities for the tables. You will need to modify these templates accordingly to change the defaults set. By default the tables are configured with a read and write capacity of 100 units. Change this as necessary. When the installation scripts run, it copies over the resource type configuration to the same directory as the main.tf template. Terraform is then able to build out the infrastructure for a given resource type. Infrastructure \u00b6 This is organized similar to the DynamoDB templates. This must be executed after the DynamoDB templates on installation and before the DynamoDB templates on tear-down (for uninstallation should you need to tear down the stack). This is structured as follows: main.tf - This is the main template with most of the infrastructure components identified. Very few (or no) changes need to be made here. This is used for ALL stacks. off-regions.tf - This outlines all of the off-region components that are required. This file has a duplicate of every region off-regions' components. Unfortunately, because Terraform lacks a great way to perform loops and iterations, we duplicate the configuration for each region. This makes the file very large and painful to edit. The sample included makes an ASSUMPTION that you will be utilizing us-west-2 as your PRIMARY REGION , and us-east-1 and eu-west-1 as your SECONDARY REGIONS . Thus, all other regions are the off-regions in our sample. You will need to alter this should you want to change the regions for your deployment. You will need to modify this template accordingly to change the defaults set. This is used for ALL stacks. If you want to specify different primary, secondary, and off-regions for a given AWS resource type, then you will need to make your own modifications to the installation scripting to leverage different templates. Per-resource type stack configurations. Included are details for S3 and Security Groups. There is a Terraform template for each resource type. This is where you need to configure a number of details. Most of the defaults values are fine and should not be changed. You will need to set the PRIMARY_REGION , and POLLING_REGIONS variables accordingly. With the exception of S3, the POLLING_REGIONS should include the primary and secondary regions in the list. You will need to review all of the variables and comments in the template to understand what they mean how they should be set. If you change the defaults, you will need to make updates as necessary. Configuration and Environment Variables \u00b6 IMPORTANT: There are many environment variables and configuration details that are required to be set. Please review this page for details on this . Next Steps \u00b6 Once you have thoroughly reviewed this section, please return back to the installation documentation .","title":"Terraform"},{"location":"installation/terraform/#historical-terraform-setup","text":"A set of sample Terraform templates are included to assist with the roll-out of the infrastructure. This is intended to be run within a Docker container (code also included). The Docker container will: Package the Historical Lambda code Run the Terraform templates to provision all of the infrastructure This is all run within an Amazon Linux Docker container. Amazon Linux is required because Historical's dependencies make use of statically linked libraries, which will fail to run in the Lambda environment unless the binaries are built on Amazon Linux. You can also use this to uninstall Historical from your environment as well. Please review each section below, as the details are very important:","title":"Historical Terraform Setup"},{"location":"installation/terraform/#structure","text":"The Terraform templates are split into multiple components: Terraform Plugins (located in terraform/terraform-plugins) DynamoDB (located in terraform/dynamodb) Infrastructure (located in terraform/infra)","title":"Structure"},{"location":"installation/terraform/#terraform-backend-configuration","text":"We make the assumption that the Terraform backend is on S3. As such, you will need an S3 bucket that resides in the Historical AWS account. It is highly recommended that you configure the Historical Terraform S3 bucket with versioning enabled. This is needed should there ever be an issue with the Terraform state. NOTE: For ALL Terraform main.tf template files, at the top of the template file is a backend region configuration. It looks like this: terraform { backend \"s3\" { // Set this to where your Terraform S3 bucket is located (using us-west-2 as the example): region = \"us-west-2\" } } You will need to set the region to where your Terraform S3 bucket resides. In our examples, we are making use of us-west-2 .","title":"Terraform Backend Configuration"},{"location":"installation/terraform/#terraform-plugins","text":"This is a Terraform template that is executed in the Docker build step. This is done to pin the Terraform plugins to the Docker container so that they need not be re-downloaded later. It is important to keep the version numbers in this doc in sync with the rest of the templates.","title":"Terraform Plugins"},{"location":"installation/terraform/#dynamodb-templates","text":"This is used to construct the Global DynamoDB tables used by Historical. This is structured as follows: main.tf - This is the main template with the components required to build out the Global DynamoDB tables for a given Historical stack. The sample included makes an ASSUMPTION that you will be utilizing us-west-2 as your PRIMARY REGION , and us-east-1 and eu-west-1 as your SECONDARY REGIONS . You will need to modify this template accordingly to change the defaults set. This is used for ALL stacks. If you want to specify different primary and secondary regions for a given AWS resource type, then you will need to make your own modifications to the installation scripting to leverage different templates. Per-resource type stack configurations. Included are details for S3 and Security Groups. There is a Terraform template for each resource type. This is where you can configure the read and write capacities for the tables. You will need to modify these templates accordingly to change the defaults set. By default the tables are configured with a read and write capacity of 100 units. Change this as necessary. When the installation scripts run, it copies over the resource type configuration to the same directory as the main.tf template. Terraform is then able to build out the infrastructure for a given resource type.","title":"DynamoDB Templates"},{"location":"installation/terraform/#infrastructure","text":"This is organized similar to the DynamoDB templates. This must be executed after the DynamoDB templates on installation and before the DynamoDB templates on tear-down (for uninstallation should you need to tear down the stack). This is structured as follows: main.tf - This is the main template with most of the infrastructure components identified. Very few (or no) changes need to be made here. This is used for ALL stacks. off-regions.tf - This outlines all of the off-region components that are required. This file has a duplicate of every region off-regions' components. Unfortunately, because Terraform lacks a great way to perform loops and iterations, we duplicate the configuration for each region. This makes the file very large and painful to edit. The sample included makes an ASSUMPTION that you will be utilizing us-west-2 as your PRIMARY REGION , and us-east-1 and eu-west-1 as your SECONDARY REGIONS . Thus, all other regions are the off-regions in our sample. You will need to alter this should you want to change the regions for your deployment. You will need to modify this template accordingly to change the defaults set. This is used for ALL stacks. If you want to specify different primary, secondary, and off-regions for a given AWS resource type, then you will need to make your own modifications to the installation scripting to leverage different templates. Per-resource type stack configurations. Included are details for S3 and Security Groups. There is a Terraform template for each resource type. This is where you need to configure a number of details. Most of the defaults values are fine and should not be changed. You will need to set the PRIMARY_REGION , and POLLING_REGIONS variables accordingly. With the exception of S3, the POLLING_REGIONS should include the primary and secondary regions in the list. You will need to review all of the variables and comments in the template to understand what they mean how they should be set. If you change the defaults, you will need to make updates as necessary.","title":"Infrastructure"},{"location":"installation/terraform/#configuration-and-environment-variables","text":"IMPORTANT: There are many environment variables and configuration details that are required to be set. Please review this page for details on this .","title":"Configuration and Environment Variables"},{"location":"installation/terraform/#next-steps","text":"Once you have thoroughly reviewed this section, please return back to the installation documentation .","title":"Next Steps"}]}